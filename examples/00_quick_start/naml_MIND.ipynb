{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAML: Neural News Recommendation with Attentive Multi-View Learning\n",
    "NAML \\[1\\] is a multi-view news recommendation approach. The core of NAML is a news encoder and a user encoder. The newsencoder is composed of a title encoder, a body encoder, a vert encoder and a subvert encoder. The CNN-based title encoder and body encoder learn title and body representations by capturing words semantic information. After getting news title, body, vert and subvert representations, an attention network is used to aggregate those vectors. In the user encoder, we learn representations of users from their browsed news. Besides, we apply additive attention to learn more informative news and user representations by selecting important words and news.\n",
    "\n",
    "## Properties of NAML:\n",
    "- NAML is a multi-view neural news recommendation approach.\n",
    "- It uses news title, news body, news vert and news subvert to get news repersentations. And it uses user historical behaviors to learn user representations.\n",
    "- NAML uses additive attention to learn informative news and user representations by selecting important words and news.\n",
    "- Due to some legal issue, MIND dataset does not release news body. Therefore, we use news abstract instead.\n",
    "\n",
    "## Data format:\n",
    "For quicker training and evaluaiton, we sample MINDdemo dataset of 5k users from [MIND small dataset](https://msnews.github.io/). The MINDdemo dataset has the same file format as MINDsmall and MINDlarge. If you want to try experiments on MINDsmall\n",
    " and MINDlarge, please change the dowload source.\n",
    " Select the MIND_type parameter from ['large', 'small', 'demo'] to choose dataset.\n",
    " \n",
    "**MINDdemo_train** is used for training, and **MINDdemo_dev** is used for evaluation. Training data and evaluation data are composed of a news file and a behaviors file. You can find more detailed data description in [MIND repo](https://github.com/msnews/msnews.github.io/blob/master/assets/doc/introduction.md)\n",
    "\n",
    "### news data\n",
    "This file contains news information including newsid, category, subcatgory, news title, news abstarct, news url and entities in news title, entities in news abstarct.\n",
    "One simple example: <br>\n",
    "\n",
    "`N46466\tlifestyle\tlifestyleroyals\tThe Brands Queen Elizabeth, Prince Charles, and Prince Philip Swear By\tShop the notebooks, jackets, and more that the royals can't live without.\thttps://www.msn.com/en-us/lifestyle/lifestyleroyals/the-brands-queen-elizabeth,-prince-charles,-and-prince-philip-swear-by/ss-AAGH0ET?ocid=chopendata\t[{\"Label\": \"Prince Philip, Duke of Edinburgh\", \"Type\": \"P\", \"WikidataId\": \"Q80976\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [48], \"SurfaceForms\": [\"Prince Philip\"]}, {\"Label\": \"Charles, Prince of Wales\", \"Type\": \"P\", \"WikidataId\": \"Q43274\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [28], \"SurfaceForms\": [\"Prince Charles\"]}, {\"Label\": \"Elizabeth II\", \"Type\": \"P\", \"WikidataId\": \"Q9682\", \"Confidence\": 0.97, \"OccurrenceOffsets\": [11], \"SurfaceForms\": [\"Queen Elizabeth\"]}]\t[]`\n",
    "<br>\n",
    "\n",
    "In general, each line in data file represents information of one piece of news: <br>\n",
    "\n",
    "`[News ID] [Category] [Subcategory] [News Title] [News Abstrct] [News Url] [Entities in News Title] [Entities in News Abstract] ...`\n",
    "\n",
    "<br>\n",
    "\n",
    "We generate a word_dict file to tranform words in news title and news abstract to word indexes, and a embedding matrix is initted from pretrained glove embeddings.\n",
    "\n",
    "### behaviors data\n",
    "One simple example: <br>\n",
    "`1\tU82271\t11/11/2019 3:28:58 PM\tN3130 N11621 N12917 N4574 N12140 N9748\tN13390-0 N7180-0 N20785-0 N6937-0 N15776-0 N25810-0 N20820-0 N6885-0 N27294-0 N18835-0 N16945-0 N7410-0 N23967-0 N22679-0 N20532-0 N26651-0 N22078-0 N4098-0 N16473-0 N13841-0 N15660-0 N25787-0 N2315-0 N1615-0 N9087-0 N23880-0 N3600-0 N24479-0 N22882-0 N26308-0 N13594-0 N2220-0 N28356-0 N17083-0 N21415-0 N18671-0 N9440-0 N17759-0 N10861-0 N21830-0 N8064-0 N5675-0 N15037-0 N26154-0 N15368-1 N481-0 N3256-0 N20663-0 N23940-0 N7654-0 N10729-0 N7090-0 N23596-0 N15901-0 N16348-0 N13645-0 N8124-0 N20094-0 N27774-0 N23011-0 N14832-0 N15971-0 N27729-0 N2167-0 N11186-0 N18390-0 N21328-0 N10992-0 N20122-0 N1958-0 N2004-0 N26156-0 N17632-0 N26146-0 N17322-0 N18403-0 N17397-0 N18215-0 N14475-0 N9781-0 N17958-0 N3370-0 N1127-0 N15525-0 N12657-0 N10537-0 N18224-0`\n",
    "<br>\n",
    "\n",
    "In general, each line in data file represents one instance of an impression. The format is like: <br>\n",
    "\n",
    "`[Impression ID] [User ID] [Impression Time] [User Click History] [Impression News]`\n",
    "\n",
    "<br>\n",
    "\n",
    "User Click History is the user historical clicked news before Impression Time. Impression News is the displayed news in an impression, which format is:<br>\n",
    "\n",
    "`[News ID 1]-[label1] ... [News ID n]-[labeln]`\n",
    "\n",
    "<br>\n",
    "Label represents whether the news is clicked by the user. All information of news in User Click History and Impression News can be found in news data file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.11 | packaged by conda-forge | (default, Aug  5 2020, 20:19:23) \n",
      "[GCC Clang 10.0.1 ]\n",
      "Tensorflow version: 1.15.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import papermill as pm\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from reco_utils.recommender.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from reco_utils.recommender.newsrec.newsrec_utils import prepare_hparams\n",
    "from reco_utils.recommender.newsrec.models.naml import NAMLModel\n",
    "from reco_utils.recommender.newsrec.io.mind_all_iterator import MINDAllIterator\n",
    "from reco_utils.recommender.newsrec.newsrec_utils import get_mind_data_set\n",
    "\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs=6\n",
    "seed=42\n",
    "MIND_type = 'demo'\n",
    "\n",
    "tmpdir = TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fengbin/Documents/1work/code/recommenders/data/naml_data\n"
     ]
    }
   ],
   "source": [
    "data_path = tmpdir.name\n",
    "data_path = os.path.abspath(os.path.join(os.getcwd(), \"../../data/naml_data\"))\n",
    "print(data_path)\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "wordEmb_file = os.path.join(data_path, \"utils\", \"embedding_all.npy\")\n",
    "userDict_file = os.path.join(data_path, \"utils\", \"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, \"utils\", \"word_dict_all.pkl\")\n",
    "vertDict_file = os.path.join(data_path, \"utils\", \"vert_dict.pkl\")\n",
    "subvertDict_file = os.path.join(data_path, \"utils\", \"subvert_dict.pkl\")\n",
    "yaml_file = os.path.join(data_path, \"utils\", r'naml.yaml')\n",
    "\n",
    "mind_url, mind_train_dataset, mind_dev_dataset, mind_utils = get_mind_data_set(MIND_type)\n",
    "\n",
    "if not os.path.exists(train_news_file):\n",
    "    download_deeprec_resources(mind_url, os.path.join(data_path, 'train'), mind_train_dataset)\n",
    "    \n",
    "if not os.path.exists(valid_news_file):\n",
    "    download_deeprec_resources(mind_url, \\\n",
    "                               os.path.join(data_path, 'valid'), mind_dev_dataset)\n",
    "if not os.path.exists(yaml_file):\n",
    "    download_deeprec_resources(r'https://recodatasets.blob.core.windows.net/newsrec/', \\\n",
    "                               os.path.join(data_path, 'utils'), mind_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "data_format=naml,iterator_type=None,support_quick_scoring=True,wordEmb_file=/Users/fengbin/Documents/1work/code/recommenders/data/naml_data/utils/embedding_all.npy,wordDict_file=/Users/fengbin/Documents/1work/code/recommenders/data/naml_data/utils/word_dict_all.pkl,userDict_file=/Users/fengbin/Documents/1work/code/recommenders/data/naml_data/utils/uid2index.pkl,vertDict_file=/Users/fengbin/Documents/1work/code/recommenders/data/naml_data/utils/vert_dict.pkl,subvertDict_file=/Users/fengbin/Documents/1work/code/recommenders/data/naml_data/utils/subvert_dict.pkl,title_size=30,body_size=50,word_emb_dim=300,word_size=None,user_num=None,vert_num=17,subvert_num=249,his_size=50,npratio=4,dropout=0.2,attention_hidden_dim=200,head_num=4,head_dim=100,cnn_activation=relu,dense_activation=relu,filter_num=400,window_size=3,vert_emb_dim=100,subvert_emb_dim=100,gru_unit=400,type=ini,user_emb_dim=50,learning_rate=0.0001,loss=cross_entropy_loss,optimizer=adam,epochs=6,batch_size=64,show_step=100000,metrics=['group_auc', 'mean_mrr', 'ndcg@5;10']\n"
     ]
    }
   ],
   "source": [
    "hparams = prepare_hparams(yaml_file, wordEmb_file=wordEmb_file, \\\n",
    "                          wordDict_file=wordDict_file, userDict_file=userDict_file, \\\n",
    "                          vertDict_file=vertDict_file, subvertDict_file=subvertDict_file, \\\n",
    "                          epochs=epochs)\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = MINDAllIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the NAML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NAMLModel(hparams, iterator, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[3,0] = 17 is not in [0, 17)\n\t [[{{node vert_encoder_1/embedding_4/embedding_lookup}}]]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-dc810c0dbad9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_eval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalid_news_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalid_behaviors_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/1work/code/recommenders/reco_utils/recommender/newsrec/models/base_model.py\u001B[0m in \u001B[0;36mrun_eval\u001B[0;34m(self, news_filename, behaviors_file)\u001B[0m\n\u001B[1;32m    309\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msupport_quick_scoring\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m             _, group_labels, group_preds = self.run_fast_eval(\n\u001B[0;32m--> 311\u001B[0;31m                 \u001B[0mnews_filename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbehaviors_file\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m             )\n\u001B[1;32m    313\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/1work/code/recommenders/reco_utils/recommender/newsrec/models/base_model.py\u001B[0m in \u001B[0;36mrun_fast_eval\u001B[0;34m(self, news_filename, behaviors_file)\u001B[0m\n\u001B[1;32m    382\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    383\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mrun_fast_eval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnews_filename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbehaviors_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 384\u001B[0;31m         \u001B[0mnews_vecs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_news\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnews_filename\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# dict(zip(news_indexes, news_vecs))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    385\u001B[0m         \u001B[0muser_vecs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_user\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnews_filename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbehaviors_file\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# dict(zip(user_indexes, user_vecs))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    386\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/1work/code/recommenders/reco_utils/recommender/newsrec/models/base_model.py\u001B[0m in \u001B[0;36mrun_news\u001B[0;34m(self, news_filename)\u001B[0m\n\u001B[1;32m    357\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtest_iterator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_news_from_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnews_filename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    358\u001B[0m         ):\n\u001B[0;32m--> 359\u001B[0;31m             \u001B[0mnews_index\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnews_vec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnews\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_data_input\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    360\u001B[0m             \u001B[0mnews_indexes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnews_index\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    361\u001B[0m             \u001B[0mnews_vecs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnews_vec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/1work/code/recommenders/reco_utils/recommender/newsrec/models/base_model.py\u001B[0m in \u001B[0;36mnews\u001B[0;34m(self, batch_news_input)\u001B[0m\n\u001B[1;32m    327\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mnews\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_news_input\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m         \u001B[0mnews_input\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_news_feature_from_iter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_news_input\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 329\u001B[0;31m         \u001B[0mnews_vec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnewsencoder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_on_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnews_input\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    330\u001B[0m         \u001B[0mnews_index\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch_news_input\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"news_index_batch\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    331\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/reco_base/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001B[0m in \u001B[0;36mpredict_on_batch\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m   1154\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1155\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_predict_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1156\u001B[0;31m     \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1157\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1158\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/reco_base/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m   3474\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3475\u001B[0m     fetched = self._callable_fn(*array_vals,\n\u001B[0;32m-> 3476\u001B[0;31m                                 run_metadata=self.run_metadata)\n\u001B[0m\u001B[1;32m   3477\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_fetch_callbacks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetched\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fetches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3478\u001B[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001B[0;32m~/anaconda3/envs/reco_base/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1470\u001B[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001B[1;32m   1471\u001B[0m                                                \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_handle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1472\u001B[0;31m                                                run_metadata_ptr)\n\u001B[0m\u001B[1;32m   1473\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1474\u001B[0m           \u001B[0mproto_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrun_metadata_ptr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: indices[3,0] = 17 is not in [0, 17)\n\t [[{{node vert_encoder_1/embedding_4/embedding_lookup}}]]"
     ]
    }
   ],
   "source": [
    "print(model.run_eval(valid_news_file, valid_behaviors_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:08, ?it/s]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[12,0] = 17 is not in [0, 17)\n\t [[{{node time_distributed_3/news_encoder/vert_encoder/embedding_4/embedding_lookup}}]]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-15-0de3248d5b3b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_news_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_behaviors_file\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mvalid_news_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalid_behaviors_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/1work/code/recommenders/reco_utils/recommender/newsrec/models/base_model.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file, test_news_file, test_behaviors_file)\u001B[0m\n\u001B[1;32m    198\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mbatch_data_input\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm_util\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    199\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 200\u001B[0;31m                 \u001B[0mstep_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_data_input\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    201\u001B[0m                 \u001B[0mstep_data_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstep_result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/1work/code/recommenders/reco_utils/recommender/newsrec/models/base_model.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, train_batch_data)\u001B[0m\n\u001B[1;32m    142\u001B[0m         \"\"\"\n\u001B[1;32m    143\u001B[0m         \u001B[0mtrain_input\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_label\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_input_label_from_iter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_batch_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 144\u001B[0;31m         \u001B[0mrslt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_on_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_input\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_label\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    145\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mrslt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/reco_base/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001B[0m in \u001B[0;36mtrain_on_batch\u001B[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001B[0m\n\u001B[1;32m   1015\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_update_sample_weight_modes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample_weights\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msample_weights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1016\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_train_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1017\u001B[0;31m       \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mins\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1018\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1019\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mreset_metrics\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/reco_base/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m   3474\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3475\u001B[0m     fetched = self._callable_fn(*array_vals,\n\u001B[0;32m-> 3476\u001B[0;31m                                 run_metadata=self.run_metadata)\n\u001B[0m\u001B[1;32m   3477\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_fetch_callbacks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetched\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fetches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3478\u001B[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001B[0;32m~/anaconda3/envs/reco_base/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1470\u001B[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001B[1;32m   1471\u001B[0m                                                \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_handle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1472\u001B[0;31m                                                run_metadata_ptr)\n\u001B[0m\u001B[1;32m   1473\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1474\u001B[0m           \u001B[0mproto_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrun_metadata_ptr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: indices[12,0] = 17 is not in [0, 17)\n\t [[{{node time_distributed_3/news_encoder/vert_encoder/embedding_4/embedding_lookup}}]]"
     ]
    }
   ],
   "source": [
    "model.fit(train_news_file, train_behaviors_file,valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18661it [01:19, 233.47it/s]\n",
      "7475it [00:30, 248.54it/s]\n",
      "7538it [00:01, 7181.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.6415, 'mean_mrr': 0.296, 'ndcg@5': 0.326, 'ndcg@10': 0.3903}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/anaconda/envs/reco_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.1). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "res_syn": {
        "group_auc": 0.6415,
        "mean_mrr": 0.296,
        "ndcg@10": 0.3903,
        "ndcg@5": 0.326
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
    "print(res_syn)\n",
    "pm.record(\"res_syn\", res_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(data_path, \"model\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "model.model.save_weights(os.path.join(model_path, \"naml_ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Predcition File\n",
    "This code segment is used to generate the prediction.zip file, which is in the same format in [MIND Competition Submission Tutorial](https://competitions.codalab.org/competitions/24122#learn_the_details-submission-guidelines).\n",
    "\n",
    "Please change the `MIND_type` parameter to `large` if you want to submit your prediction to [MIND Competition](https://msnews.github.io/competition.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18661it [01:19, 233.52it/s]\n",
      "7475it [00:29, 252.92it/s]\n",
      "7538it [00:01, 6852.74it/s]\n"
     ]
    }
   ],
   "source": [
    "group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7538it [00:00, 43079.81it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(data_path, 'prediction.txt'), 'w') as f:\n",
    "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
    "        impr_index += 1\n",
    "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
    "        pred_rank = '[' + ','.join([str(i) for i in pred_rank]) + ']'\n",
    "        f.write(' '.join([str(impr_index), pred_rank])+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = zipfile.ZipFile(os.path.join(data_path, 'prediction.zip'), 'w', zipfile.ZIP_DEFLATED)\n",
    "f.write(os.path.join(data_path, 'prediction.txt'), arcname='prediction.txt')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\\[1\\] Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang and Xing Xie: Neural News Recommendation with Attentive Multi-View Learning, IJCAI 2019<br>\n",
    "\\[2\\] Wu, Fangzhao, et al. \"MIND: A Large-scale Dataset for News Recommendation\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. https://msnews.github.io/competition.html <br>\n",
    "\\[3\\] GloVe: Global Vectors for Word Representation. https://nlp.stanford.edu/projects/glove/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "name": "reco_base",
   "language": "python",
   "display_name": "Python (reco)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}